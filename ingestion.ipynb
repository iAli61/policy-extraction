{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/alibina/repo/MunichRe/policy-extraction\n",
      "Added to path: /home/alibina/repo/MunichRe/policy-extraction\n",
      "Python path: ['/home/alibina/repo/MunichRe/policy-extraction', '/home/alibina/miniconda3/envs/ai_scientist/lib/python311.zip', '/home/alibina/miniconda3/envs/ai_scientist/lib/python3.11', '/home/alibina/miniconda3/envs/ai_scientist/lib/python3.11/lib-dynload', '', '/home/alibina/miniconda3/envs/ai_scientist/lib/python3.11/site-packages']\n",
      "Module path exists: True\n",
      "Saved image: markdown_files/images/image_2840.png\n",
      "Saved image: markdown_files/images/image_2616.png\n",
      "Converted: test_files/test.docx → markdown_files/test.md\n",
      "Created HTML preview: markdown_files/test.preview.html\n",
      "Created HTML preview: markdown_files/test.preview.html\n",
      "Successfully converted: test_files/test.docx\n",
      "Converted 1 files. Output files are in the markdown_files directory.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get absolute path to the directory containing word_to_markdown.py\n",
    "project_dir = os.path.abspath('.')\n",
    "sys.path.insert(0, project_dir)  # Insert at beginning of path\n",
    "\n",
    "# Debug to verify paths\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Added to path: {project_dir}\")\n",
    "print(f\"Python path: {sys.path}\")\n",
    "\n",
    "# Check if file exists\n",
    "module_path = os.path.join(project_dir, \"word_to_markdown.py\")\n",
    "print(f\"Module path exists: {os.path.exists(module_path)}\")\n",
    "\n",
    "# Import the converter from your script\n",
    "from word_to_markdown import WordToMarkdownConverter\n",
    "\n",
    "volume_path = \"test_files\"\n",
    "\n",
    "# Get all PDFs\n",
    "all_docx = glob.glob(f\"{volume_path}/*.docx\")\n",
    "\n",
    "# Initialize the converter\n",
    "converter = WordToMarkdownConverter(\n",
    "    preserve_tables=True,\n",
    "    preserve_images=True,\n",
    "    preserve_lists=True\n",
    ")\n",
    "\n",
    "# Create an output directory for the converted markdown files\n",
    "output_dir = \"markdown_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert each DOCX file to Markdown\n",
    "converted_files = []\n",
    "for docx_file in all_docx:\n",
    "    try:\n",
    "        # Define output path (same filename but with .md extension in the output directory)\n",
    "        docx_filename = os.path.basename(docx_file)\n",
    "        output_path = os.path.join(output_dir, Path(docx_filename).with_suffix('.md'))\n",
    "        \n",
    "        # Perform the conversion\n",
    "        converted_file = converter.convert_file(docx_file, output_path)\n",
    "        converted_files.append(converted_file)\n",
    "        print(f\"Successfully converted: {docx_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to convert {docx_file}: {e}\")\n",
    "\n",
    "print(f\"Converted {len(converted_files)} files. Output files are in the {output_dir} directory.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/chunk_caption_index_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# add the markdown-indexer/src directory to the Python path\n",
    "sys.path.insert(0, 'markdown-indexer')\n",
    "\n",
    "from src.markdown_processor import MarkdownProcessor\n",
    "from src.embeddings import EmbeddingGenerator\n",
    "from src.storage.faiss_storage import FaissStorage\n",
    "import os\n",
    "\n",
    "def index_markdown_with_faiss(markdown_text, \n",
    "                              model_name, \n",
    "                              chunk_size= 4000, \n",
    "                              chunk_overlap=200,\n",
    "                              max_table_size=2000,\n",
    "                              output_dir=None):\n",
    "\n",
    "    # check if output_dir is provided\n",
    "    if output_dir:\n",
    "        output_chunks = f\"{output_dir}/chunks.jsonl\"\n",
    "        output_index = f\"{output_dir}/faiss_index\"\n",
    "        output_documents = f\"{output_dir}/documents.pkl\"\n",
    "    else:\n",
    "        output_chunks = None\n",
    "        output_index = None\n",
    "        output_documents = None\n",
    "    # Check if the model name is provided\n",
    "    if not model_name:\n",
    "        raise ValueError(\"Model name must be provided.\")\n",
    "    # Check if the markdown text is provided\n",
    "    if not markdown_text:\n",
    "        raise ValueError(\"Markdown text must be provided.\")\n",
    "    # Check if the output directory exists\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Initialize the Markdown processor\n",
    "    processor = MarkdownProcessor(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        max_table_size=max_table_size\n",
    "    )\n",
    "    \n",
    "    # Parse and chunk the markdown text - using the correct method flow\n",
    "    parsed_blocks = processor.parse_markdown(markdown_text)\n",
    "    chunks = processor.chunk_text(parsed_blocks)\n",
    "\n",
    "    # save chunks to a file if output_chunks is provided\n",
    "    if output_chunks:\n",
    "        processor.save_chunks(chunks, output_chunks)\n",
    "\n",
    "    \n",
    "    # Initialize the embedding generator\n",
    "    embedding_generator = EmbeddingGenerator(model_name=model_name)\n",
    "    \n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = [embedding_generator.generate_embeddings(chunk['content']) for chunk in chunks]\n",
    "    \n",
    "    # Initialize FAISS storage with the appropriate dimension\n",
    "    dimension = embeddings[0].shape[1]  # Assuming all embeddings have the same dimension\n",
    "    faiss_storage = FaissStorage(dimension=dimension)\n",
    "    \n",
    "    # Index the embeddings\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        faiss_storage.add((embedding, chunk['content']))\n",
    "\n",
    "    # Save the FAISS index and documents\n",
    "    if output_index and output_documents:\n",
    "        faiss_storage.save(output_index, output_documents)\n",
    "    \n",
    "    print(f\"Indexed {len(chunks)} chunks into FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 2 chunks into FAISS.\n"
     ]
    }
   ],
   "source": [
    "# read the markdown text from a file\n",
    "with open(\"/home/azureuser/policy-extraction/markdown_files/test.md\", \"r\") as f:\n",
    "    markdown_text = f.read()\n",
    "\n",
    "# Specify the Hugging Face model name\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Index the markdown text\n",
    "index_markdown_with_faiss(\n",
    "    markdown_text,\n",
    "    model_name,\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=200,\n",
    "    max_table_size=2000,\n",
    "    output_dir=\"output\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chunked data\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "def load_chunked_data(file_path):\n",
    "    \"\"\"\n",
    "    Load chunked data from a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing chunked data.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_faiss_index(index_path, documents_path):\n",
    "    \"\"\"\n",
    "    Load a FAISS index and documents.\n",
    "\n",
    "    Args:\n",
    "        index_path (str): Path to the FAISS index file.\n",
    "        documents_path (str): Path to the documents file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: FAISS index and documents.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(index_path):\n",
    "        raise FileNotFoundError(f\"Index file not found: {index_path}\")\n",
    "\n",
    "    if not os.path.exists(documents_path):\n",
    "        raise FileNotFoundError(f\"Documents file not found: {documents_path}\")\n",
    "\n",
    "    index = faiss.read_index(index_path)\n",
    "    documents = pd.read_pickle(documents_path)\n",
    "\n",
    "    return index, documents\n",
    "\n",
    "def search_faiss_index(index, query_vector, k=5):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the nearest neighbors of a query vector.\n",
    "\n",
    "    Args:\n",
    "        index (faiss.Index): FAISS index.\n",
    "        query_vector (numpy.ndarray): Query vector.\n",
    "        k (int): Number of nearest neighbors to return.\n",
    "    \"\"\"\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file = \"output/chunks.jsonl\"\n",
    "faiss_index_file = \"output/faiss_index\"\n",
    "documents_file = \"output/documents.pkl\"\n",
    "\n",
    "# Load the chunked data\n",
    "chunked_data = load_chunked_data(jsonl_file)\n",
    "# Load the FAISS index and documents\n",
    "index, documents = load_faiss_index(faiss_index_file, documents_file)\n",
    "# Example query vector (replace with your own)\n",
    "query_vector = index.reconstruct(0)  # Replace with your own query vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- Styles for better image display -->\n",
      "<style>\n",
      "img.markdown-image {\n",
      "  display: block;\n",
      "  max-width: 100%;\n",
      "  height: auto;\n",
      "  margin: 20px 0;\n",
      "  border-radius: 5px;\n",
      "}\n",
      "</style>\n",
      "\n",
      "**Annotation Benchmarking**\n",
      "\n",
      "**Chemical Reaction Figures**\n",
      "\n",
      "**Models:**\n",
      "\n",
      "- AI4Chem/ChemVLM-8B\n",
      "- GPT-4 turbo-2024-04-09\n",
      "\n",
      "**Input prompt:**\n",
      "\n",
      "- You are a chemical annotation assistant. Analyze chemical reaction images, and annotate all relevant information such as reactants, products, catalysts, conditions, and mechanisms. Don't include any reactions or formulas in your annotations. Explain what they mean and serve for. Ensure chemical accuracy, use standard conventions, and maintain completeness and clarity in the annotations. **Sample document:**\n",
      "\n",
      "- ChemBioChem - 2020 - Norvaiša - Porphyrins as Colorimetric and Photometric Biosensors in Modern Bioanalytical Systems.pdf\n"
     ]
    }
   ],
   "source": [
    "print(chunked_data[0]['content'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunk_caption_index_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
